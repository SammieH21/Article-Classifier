{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import os\n",
    "import string\n",
    "import re\n",
    "import io\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from zipfile import ZipFile\n",
    "import requests\n",
    "import gzip\n",
    "import tensorflow as tf\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"bbc\"):\n",
    "    !wget \"http://mlg.ucd.ie/files/datasets/bbc-fulltext.zip\"\n",
    "    ZipFile(\"bbc-fulltext.zip\").extractall()\n",
    "if not os.path.isfile(\"glove.6B.50d.txt.gz\"):\n",
    "    !wget \"https://github.com/kmr0877/IMDB-Sentiment-Classification-CBOW-Model/blob/master/glove.6B.50d.txt.gz?raw=true\" -O \"glove.6B.50d.txt.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_text_files = os.listdir(\"bbc/business\")\n",
    "entertainment_text_files = os.listdir(\"bbc/entertainment\")\n",
    "politics_text_files = os.listdir(\"bbc/politics\")\n",
    "tech_text_files = os.listdir(\"bbc/tech\")\n",
    "sports_text_files = os.listdir(\"bbc/sport\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(file,directory):\n",
    "    file_path = directory + \"/\" + file\n",
    "    #print(file_path)\n",
    "    try:\n",
    "        with open(file_path,'r') as f:\n",
    "            text = f.read()\n",
    "\n",
    "    # at least one file is ISO-8859-14 encoded. That could cause some issues unless accounted for\n",
    "    except UnicodeDecodeError:\n",
    "        with open(file_path,'r',encoding=\"ISO-8859-14\") as f:\n",
    "            text = f.read()\n",
    "    return text\n",
    "\n",
    "business_texts = [read_text(text_file,directory=\"bbc/business\") for text_file in business_text_files]\n",
    "entertainment_texts = [read_text(text_file,directory=\"bbc/entertainment\") for text_file in entertainment_text_files]\n",
    "politics_texts = [read_text(text_file,directory=\"bbc/politics\") for text_file in politics_text_files]\n",
    "tech_texts = [read_text(text_file,directory=\"bbc/tech\") for text_file in tech_text_files]\n",
    "sport_texts = [read_text(text_file,directory=\"bbc/sport\") for text_file in sports_text_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = [business_texts, entertainment_texts, politics_texts, tech_texts, sport_texts]\n",
    "text_idx = [0,1,2,3,4]\n",
    "article_types = [\"business\",\"entertainment\",\"politics\",\"tech\",\"sports\"]\n",
    "class_dict = dict(zip(text_idx,article_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\",\n",
    "             \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\",\n",
    "             \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\",\n",
    "             \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\",\n",
    "             \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\",\n",
    "             \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\",\n",
    "             \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\",\n",
    "              \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\",\n",
    "             \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\",\n",
    "             \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\",\n",
    "             \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\",\n",
    "             \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\",\n",
    "             \"yourself\", \"yourselves\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([[text,label] for (texts, label) in zip(all_texts,text_idx) for text in texts],columns=[\"text\",\"label\"])\n",
    "df_train, df_test = train_test_split(df, train_size=.8,random_state=111)\n",
    "df_train = df_train.copy()\n",
    "df_test = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    processed_text = \" \".join([word for word in re.sub(\"\\.+\", \". \", re.sub(\"[\\(\\)\\[\\]\\\"\\']\",\"\",text.replace(\"\\n|\\w+\", \" \"))).split(\" \") if word.lower().strip() not in stopwords])\n",
    "    return processed_text\n",
    "\n",
    "def remove_punctuation(text):\n",
    "     return text.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = [sentence.strip() for text in df_train.text.values for sentence in text.split(\".\") if sentence.strip() != \"\"]\n",
    "\n",
    "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(all_sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "reverse_idx = {value :key for (key, value) in word_index.items()}\n",
    "\n",
    "maxlen = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['tokenized'] = df_train.text.apply(lambda text: tokenizer.texts_to_sequences([text])[0])\n",
    "df_train[\"tokenized\"] = [sequence for sequence in pad_sequences(df_train.tokenized.values,maxlen=maxlen,truncating=\"post\",padding=\"post\")]\n",
    "\n",
    "df_test['tokenized'] = df_test.text.apply(lambda text: tokenizer.texts_to_sequences([text])[0])\n",
    "df_test[\"tokenized\"] = [sequence for sequence in pad_sequences(df_test.tokenized.values,maxlen=maxlen,truncating=\"post\",padding=\"post\")]\n",
    "train_X = np.vstack(df_train[\"tokenized\"].values)\n",
    "test_X = np.vstack(df_test[\"tokenized\"].values)\n",
    "\n",
    "train_y = to_categorical(df_train.label.values)\n",
    "test_y = to_categorical(df_test.label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(\"glove.6B.50d.txt.gz\", 'r') as f:\n",
    "    embedding_list = f.read().decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vectors = {}\n",
    "for embedding_line in embedding_list.split(\"\\n\"):\n",
    "    embedding_split = embedding_line.split(\" \")\n",
    "    embedding_vectors[embedding_split[0]] = embedding_split[1:]\n",
    "\n",
    "vocab_size = len(word_index.keys()) + 1\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size,50))\n",
    "\n",
    "for word,i  in word_index.items():\n",
    "    vector = embedding_vectors.get(word)\n",
    "    if vector is not None:\n",
    "        embedding_matrix[i,:] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_index.keys()) + 1\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size,50))\n",
    "\n",
    "for word,i  in word_index.items():\n",
    "    vector = embedding_vectors.get(word)\n",
    "    if vector is not None:\n",
    "        embedding_matrix[i,:] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_17 (Embedding)     (None, 500, 50)           1481250   \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 491, 64)           32064     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 122, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 118, 96)           30816     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_17  (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 15)                1455      \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 5)                 80        \n",
      "=================================================================\n",
      "Total params: 1,545,665\n",
      "Trainable params: 64,415\n",
      "Non-trainable params: 1,481,250\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Model 1 ##\n",
    "\n",
    "model1 = keras.models.Sequential([keras.layers.Embedding(vocab_size, 50, input_length = maxlen, weights= [embedding_matrix],\n",
    "                                                        trainable=False, mask_zero=True),\n",
    "                                 keras.layers.Conv1D(64, 10,activation='relu'),\n",
    "                                 keras.layers.MaxPooling1D(4),\n",
    "                                 keras.layers.Conv1D(96, 5, activation='relu'),\n",
    "                                 keras.layers.GlobalAveragePooling1D(),\n",
    "                                 keras.layers.Dense(15,activation=\"relu\"),\n",
    "                                 keras.layers.Dropout(.2),\n",
    "                                 keras.layers.Dense(5, activation=\"softmax\")\n",
    "                                ])\n",
    "\n",
    "model1.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\",metrics=['accuracy'])\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/23\n",
      "55/55 [==============================] - 3s 42ms/step - loss: 1.4925 - accuracy: 0.3001 - val_loss: 1.1700 - val_accuracy: 0.5144\n",
      "Epoch 2/23\n",
      "55/55 [==============================] - 2s 38ms/step - loss: 1.1127 - accuracy: 0.5814 - val_loss: 0.9527 - val_accuracy: 0.7163\n",
      "Epoch 3/23\n",
      "55/55 [==============================] - 2s 37ms/step - loss: 0.9515 - accuracy: 0.6949 - val_loss: 0.8320 - val_accuracy: 0.7548\n",
      "Epoch 4/23\n",
      "55/55 [==============================] - 2s 38ms/step - loss: 0.8818 - accuracy: 0.6960 - val_loss: 0.7385 - val_accuracy: 0.7115\n",
      "Epoch 5/23\n",
      "55/55 [==============================] - 2s 37ms/step - loss: 0.6534 - accuracy: 0.7332 - val_loss: 0.3422 - val_accuracy: 0.9375\n",
      "Epoch 6/23\n",
      "55/55 [==============================] - 2s 39ms/step - loss: 0.4361 - accuracy: 0.8267 - val_loss: 0.2070 - val_accuracy: 0.9543\n",
      "Epoch 7/23\n",
      "55/55 [==============================] - 2s 38ms/step - loss: 0.3387 - accuracy: 0.8677 - val_loss: 0.1882 - val_accuracy: 0.9399\n",
      "Epoch 8/23\n",
      "55/55 [==============================] - 2s 38ms/step - loss: 0.2967 - accuracy: 0.8797 - val_loss: 0.1407 - val_accuracy: 0.9712\n",
      "Epoch 9/23\n",
      "55/55 [==============================] - 2s 38ms/step - loss: 0.2525 - accuracy: 0.9092 - val_loss: 0.1643 - val_accuracy: 0.9471\n",
      "Epoch 10/23\n",
      "55/55 [==============================] - 2s 39ms/step - loss: 0.1992 - accuracy: 0.9302 - val_loss: 0.1469 - val_accuracy: 0.9591\n",
      "Epoch 11/23\n",
      "55/55 [==============================] - 2s 40ms/step - loss: 0.2360 - accuracy: 0.9240 - val_loss: 0.1427 - val_accuracy: 0.9567\n",
      "Epoch 12/23\n",
      "55/55 [==============================] - 2s 39ms/step - loss: 0.1692 - accuracy: 0.9313 - val_loss: 0.1607 - val_accuracy: 0.9543\n",
      "Epoch 13/23\n",
      "55/55 [==============================] - 2s 39ms/step - loss: 0.1774 - accuracy: 0.9394 - val_loss: 0.1180 - val_accuracy: 0.9567\n",
      "Epoch 14/23\n",
      "55/55 [==============================] - 2s 40ms/step - loss: 0.1297 - accuracy: 0.9435 - val_loss: 0.0958 - val_accuracy: 0.9712\n",
      "Epoch 15/23\n",
      "55/55 [==============================] - 2s 38ms/step - loss: 0.1423 - accuracy: 0.9482 - val_loss: 0.2162 - val_accuracy: 0.9399\n",
      "Epoch 16/23\n",
      "55/55 [==============================] - 2s 38ms/step - loss: 0.1685 - accuracy: 0.9267 - val_loss: 0.1554 - val_accuracy: 0.9543\n",
      "Epoch 17/23\n",
      "55/55 [==============================] - 2s 38ms/step - loss: 0.1245 - accuracy: 0.9434 - val_loss: 0.1282 - val_accuracy: 0.9639\n",
      "Epoch 18/23\n",
      "55/55 [==============================] - 2s 38ms/step - loss: 0.1143 - accuracy: 0.9505 - val_loss: 0.1018 - val_accuracy: 0.9712\n",
      "Epoch 19/23\n",
      "55/55 [==============================] - 2s 38ms/step - loss: 0.1253 - accuracy: 0.9488 - val_loss: 0.1265 - val_accuracy: 0.9663\n",
      "Epoch 20/23\n",
      "55/55 [==============================] - 2s 39ms/step - loss: 0.1021 - accuracy: 0.9479 - val_loss: 0.1719 - val_accuracy: 0.9615\n",
      "Epoch 21/23\n",
      "55/55 [==============================] - 2s 38ms/step - loss: 0.0912 - accuracy: 0.9593 - val_loss: 0.1068 - val_accuracy: 0.9688\n",
      "Epoch 22/23\n",
      "55/55 [==============================] - 2s 39ms/step - loss: 0.0779 - accuracy: 0.9687 - val_loss: 0.1420 - val_accuracy: 0.9663\n",
      "Epoch 23/23\n",
      "55/55 [==============================] - 2s 38ms/step - loss: 0.0744 - accuracy: 0.9713 - val_loss: 0.0740 - val_accuracy: 0.9784\n",
      "INFO:tensorflow:Assets written to: cnn-model-glove/assets\n"
     ]
    }
   ],
   "source": [
    "model1.fit(train_X, train_y,validation_data=(test_X,test_y),\n",
    "            epochs=23, batch_size=32, steps_per_epoch= 55,validation_steps=32,validation_batch_size=13,\n",
    "          workers=5)\n",
    "\n",
    "model1.save(\"cnn-model-glove\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 500, 50)           1481250   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_3 ( (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 22)                1122      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 22)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 11)                253       \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 11)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 5)                 60        \n",
      "=================================================================\n",
      "Total params: 1,482,685\n",
      "Trainable params: 1,482,685\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Model 2 ##\n",
    "model2 = keras.models.Sequential([keras.layers.Embedding(vocab_size, 50, input_length = maxlen,\n",
    "                                                        mask_zero=True),\n",
    "                                 keras.layers.GlobalAveragePooling1D(),\n",
    "                                 keras.layers.Dense(22,activation=\"relu\"),\n",
    "                                 keras.layers.Dropout(.2),\n",
    "                                 keras.layers.Dense(11,activation=\"relu\"),\n",
    "                                 keras.layers.Dropout(.2),\n",
    "                                 keras.layers.Dense(5, activation=\"softmax\")\n",
    "                                ])\n",
    "\n",
    "model2.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\",metrics=['accuracy'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "55/55 [==============================] - 2s 21ms/step - loss: 1.6026 - accuracy: 0.2720 - val_loss: 1.5544 - val_accuracy: 0.5505\n",
      "Epoch 2/20\n",
      "55/55 [==============================] - 1s 18ms/step - loss: 1.5006 - accuracy: 0.5067 - val_loss: 1.2737 - val_accuracy: 0.6274\n",
      "Epoch 3/20\n",
      "55/55 [==============================] - 1s 17ms/step - loss: 1.1666 - accuracy: 0.5829 - val_loss: 0.9170 - val_accuracy: 0.7452\n",
      "Epoch 4/20\n",
      "55/55 [==============================] - 1s 16ms/step - loss: 0.8713 - accuracy: 0.6953 - val_loss: 0.6262 - val_accuracy: 0.8798\n",
      "Epoch 5/20\n",
      "55/55 [==============================] - 1s 16ms/step - loss: 0.6023 - accuracy: 0.8193 - val_loss: 0.4211 - val_accuracy: 0.9447\n",
      "Epoch 6/20\n",
      "55/55 [==============================] - 1s 16ms/step - loss: 0.4138 - accuracy: 0.9069 - val_loss: 0.3066 - val_accuracy: 0.9495\n",
      "Epoch 7/20\n",
      "55/55 [==============================] - 1s 17ms/step - loss: 0.2972 - accuracy: 0.9266 - val_loss: 0.2259 - val_accuracy: 0.9639\n",
      "Epoch 8/20\n",
      "55/55 [==============================] - 1s 20ms/step - loss: 0.2353 - accuracy: 0.9421 - val_loss: 0.1847 - val_accuracy: 0.9615\n",
      "Epoch 9/20\n",
      "55/55 [==============================] - 1s 17ms/step - loss: 0.1656 - accuracy: 0.9680 - val_loss: 0.1570 - val_accuracy: 0.9639\n",
      "Epoch 10/20\n",
      "55/55 [==============================] - 1s 16ms/step - loss: 0.1604 - accuracy: 0.9559 - val_loss: 0.1481 - val_accuracy: 0.9615\n",
      "Epoch 11/20\n",
      "55/55 [==============================] - 1s 19ms/step - loss: 0.1431 - accuracy: 0.9620 - val_loss: 0.1360 - val_accuracy: 0.9639\n",
      "Epoch 12/20\n",
      "55/55 [==============================] - 1s 19ms/step - loss: 0.1049 - accuracy: 0.9739 - val_loss: 0.1197 - val_accuracy: 0.9736\n",
      "Epoch 13/20\n",
      "55/55 [==============================] - 1s 17ms/step - loss: 0.0970 - accuracy: 0.9784 - val_loss: 0.1243 - val_accuracy: 0.9639\n",
      "Epoch 14/20\n",
      "55/55 [==============================] - 1s 22ms/step - loss: 0.1001 - accuracy: 0.9733 - val_loss: 0.1207 - val_accuracy: 0.9639\n",
      "Epoch 15/20\n",
      "55/55 [==============================] - 1s 22ms/step - loss: 0.0822 - accuracy: 0.9798 - val_loss: 0.1113 - val_accuracy: 0.9688\n",
      "Epoch 16/20\n",
      "55/55 [==============================] - 1s 17ms/step - loss: 0.0732 - accuracy: 0.9813 - val_loss: 0.1047 - val_accuracy: 0.9736\n",
      "Epoch 17/20\n",
      "55/55 [==============================] - 1s 21ms/step - loss: 0.0766 - accuracy: 0.9847 - val_loss: 0.1015 - val_accuracy: 0.9736\n",
      "Epoch 18/20\n",
      "55/55 [==============================] - 1s 18ms/step - loss: 0.0736 - accuracy: 0.9797 - val_loss: 0.1075 - val_accuracy: 0.9712\n",
      "Epoch 19/20\n",
      "55/55 [==============================] - 1s 18ms/step - loss: 0.0624 - accuracy: 0.9794 - val_loss: 0.1088 - val_accuracy: 0.9736\n",
      "Epoch 20/20\n",
      "55/55 [==============================] - 1s 20ms/step - loss: 0.0670 - accuracy: 0.9819 - val_loss: 0.1040 - val_accuracy: 0.9736\n",
      "INFO:tensorflow:Assets written to: cnn-model-gloveless/assets\n"
     ]
    }
   ],
   "source": [
    "model2.fit(train_X, train_y,validation_data=(test_X,test_y),\n",
    "            epochs=20, batch_size=32, steps_per_epoch= 55,\n",
    "           validation_steps=32,validation_batch_size=13,\n",
    "           workers=5)\n",
    "\n",
    "model2.save(\"cnn-model-gloveless\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model2.layers[0].get_weights()[0]\n",
    "\n",
    "word_vectors = {reverse_idx[i]:model2.layers[0].weights[0][i].numpy() for i in range(1, vocab_size)}\n",
    "\n",
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for word, vector in word_vectors.items():\n",
    "    if not np.all(vector == 0):\n",
    "        out_m.write(word + \"\\n\")\n",
    "        out_v.write('\\t'.join([str(x) for x in vector]) + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, model,verbose=True):\n",
    "    if verbose:\n",
    "        print(\"#####################################\\nAnalyzing Statement:\\n\"+text)\n",
    "    processed_text = process_text(remove_punctuation(text))\n",
    "    tokenized_text = tokenizer.texts_to_sequences([processed_text])[0]\n",
    "    padded_sequence = pad_sequences([tokenized_text],maxlen=maxlen,truncating=\"post\",padding=\"post\")\n",
    "    likelihoods = model.predict(padded_sequence)[0]\n",
    "    idx = np.argmax(likelihoods)\n",
    "    highest_probability = likelihoods[idx]\n",
    "    class_prediction = class_dict[idx]\n",
    "    if verbose:\n",
    "        print(\"\\nClass:\",class_prediction,\"\\nLikelihood:\",str(highest_probability*100)+\"%\")\n",
    "        print(\"#####################################\\n\\n\")\n",
    "    return class_prediction, highest_probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################################\n",
      "Analyzing Statement:\n",
      "Liverpool wins the match!\n",
      "\n",
      "Class: sports \n",
      "Likelihood: 100.0%\n",
      "#####################################\n",
      "\n",
      "\n",
      "#####################################\n",
      "Analyzing Statement:\n",
      "TV\n",
      "\n",
      "Class: entertainment \n",
      "Likelihood: 100.0%\n",
      "#####################################\n",
      "\n",
      "\n",
      "#####################################\n",
      "Analyzing Statement:\n",
      "Democracy\n",
      "\n",
      "Class: politics \n",
      "Likelihood: 99.9991774559021%\n",
      "#####################################\n",
      "\n",
      "\n",
      "#####################################\n",
      "Analyzing Statement:\n",
      "nvidia graphics card\n",
      "\n",
      "Class: tech \n",
      "Likelihood: 99.99639987945557%\n",
      "#####################################\n",
      "\n",
      "\n",
      "#####################################\n",
      "Analyzing Statement:\n",
      "video driver\n",
      "\n",
      "Class: tech \n",
      "Likelihood: 99.95846152305603%\n",
      "#####################################\n",
      "\n",
      "\n",
      "#####################################\n",
      "Analyzing Statement:\n",
      "luxury\n",
      "\n",
      "Class: business \n",
      "Likelihood: 100.0%\n",
      "#####################################\n",
      "\n",
      "\n",
      "#####################################\n",
      "Analyzing Statement:\n",
      "stocks\n",
      "\n",
      "Class: business \n",
      "Likelihood: 99.99998807907104%\n",
      "#####################################\n",
      "\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "predict(\"Liverpool wins the match!\",model=model2)\n",
    "predict(\"TV\",model=model2)\n",
    "predict(\"Democracy\",model=model2)\n",
    "predict(\"nvidia graphics card\",model=model2)\n",
    "predict(\"video driver\",model=model2)\n",
    "predict(\"luxury\",model=model2)\n",
    "predict(\"stocks\",model=model2)\n",
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
